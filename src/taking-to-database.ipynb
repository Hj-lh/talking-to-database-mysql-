{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev1\\AI\\Sql-generator\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connet to MySQL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sql_password = os.getenv('SQL_PASSWORD')\n",
    "db = SQLDatabase.from_uri(\"mysql+mysqlconnector://root:{sql_password}@localhost:3306/collage\")\n",
    "def get_schema(_):\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype =\"auto\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "if device == torch.device(\"cpu\"):\n",
    "    torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(user_question, schema):\n",
    "    \n",
    "    \n",
    "    prompt = f\"Based on the schema {schema}, write an SQL query for the question: {user_question}\\nSQL:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=80, num_return_sequences=1)\n",
    "    query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the SQL query from the model's output\n",
    "    if \"SQL:\" in query:\n",
    "        query = query.split(\"SQL:\")[1].strip()\n",
    "    \n",
    "\n",
    "    return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query_result,user_question):\n",
    "    \n",
    "    input_text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "    Example 1:\n",
    "    Instruction:\n",
    "    You are interacting with a user who is asking you questions about a database. Based on the question and the SQL query result, generate a simple response that directly answers the user's question based on the SQL query.\n",
    "    ### Input:\n",
    "    What subjects are there? and SQL query answer [('english',), ('Science',)]\n",
    "    ### Response: The subjects available are English and Science.\n",
    "\n",
    "    Example 2:\n",
    "    ### Instruction:\n",
    "    You are interacting with a user who is asking you questions about a database. Based on the question and the SQL query result, generate a simple response that directly answers the user's question based on the SQL query.\n",
    "\n",
    "    ### Input:\n",
    "    where is bob brown location? and SQL query answer [('ROOM 101',)]\n",
    "    ### Response: bob brown is at ROOM 101.\n",
    "\n",
    "    now your turn:\n",
    "    ### Instruction:\n",
    "    You are interacting with a user who is asking you questions about a database. Based on the question and the SQL query result, generate a simple response that directly answers the user's question based on the SQL query.\n",
    "    ### Input: \n",
    "    {user_question} and SQL query answer {query_result}\n",
    "    ### Response:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=150, eos_token_id = tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_start = \"Response:\"\n",
    "    response_part = response.split(response_start)[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    \n",
    "    return response_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "d:\\Dev1\\AI\\Sql-generator\\myenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL Query: SELECT location FROM professors WHERE professor_name = 'John Smith';\n",
      "____________________________________________________________\n",
      "Query Result: [('Room 101',)]\n",
      "____________________________________________________________\n",
      "Response: john smith's office is at ROOM 101.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"where is john smith office location ?\"\n",
    "# connection = get_db_connection()\n",
    "# schema = get_db_schema(connection)\n",
    "schema = get_schema(None)\n",
    "print(\"============================================================\")\n",
    "sql_query = generate_sql_query(user_question, get_schema(_))\n",
    "print(f\"Generated SQL Query: {sql_query}\")\n",
    "print(\"____________________________________________________________\")\n",
    "# query_result = execute_query(connection, sql_query)\n",
    "query_result = run_query(sql_query)\n",
    "print(f\"Query Result: {query_result}\")\n",
    "print(\"____________________________________________________________\")\n",
    "\n",
    "\n",
    "response = generate_response(query_result,user_question)\n",
    "print(f\"Response: {response}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
